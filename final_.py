# -*- coding: utf-8 -*-
"""final .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_pwWYS8DnAT1tws136dxakxfYVES3HIo
"""

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/post natal data.csv")
# This dataset link uploaded in github. incase showing 404 error go to my github profile go in dataset select row data and paste the URL

# Drop unnecessary columns
data_cleaned = data.drop(columns=['Timestamp'])  # Drop Timestamp as it's not useful for prediction

# Encode categorical data into numerical values
encoder = LabelEncoder()
for column in data_cleaned.columns:
    data_cleaned[column] = data_cleaned[column].fillna('Unknown')  # Fill missing values
    data_cleaned[column] = encoder.fit_transform(data_cleaned[column])

# Define features (X) and target (y)
X = data_cleaned.drop(columns=['Feeling sad or Tearful'])  # Replace target column as needed
y = data_cleaned['Feeling sad or Tearful']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
random_forest_model = RandomForestClassifier(random_state=42)

# Train the model
random_forest_model.fit(X_train, y_train)

# Make predictions
y_pred = random_forest_model.predict(X_test)

# Evaluate the model
print("Classification Report:\n", classification_report(y_test, y_pred))

# Calculate and print the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy Score: {accuracy:.2f}")

# Feature Importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': random_forest_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:\n", feature_importance)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np

# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/post natal data.csv")  # Update the path as needed

# Drop unnecessary columns
data_cleaned = data.drop(columns=['Timestamp'])  # Drop Timestamp as it's not useful for prediction

# Encode categorical data into numerical values
encoder = LabelEncoder()
for column in data_cleaned.columns:
    data_cleaned[column] = data_cleaned[column].fillna('Unknown')  # Fill missing values
    data_cleaned[column] = encoder.fit_transform(data_cleaned[column])

# Define features (X) and target (y)
X = data_cleaned.drop(columns=['Feeling sad or Tearful'])  # Replace target column as needed
y = data_cleaned['Feeling sad or Tearful']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the models
random_forest_model = RandomForestClassifier(random_state=42)
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
logistic_model = LogisticRegression(max_iter=1000, random_state=42)

# Train the models
random_forest_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)
logistic_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = random_forest_model.predict_proba(X_test)[:, 1]
y_pred_xgb = xgb_model.predict_proba(X_test)[:, 1]
y_pred_logistic = logistic_model.predict_proba(X_test)[:, 1]

# Hybrid model: Weighted average ensemble
weights = [0.4, 0.4, 0.2]  # Assign weights to each model
ensemble_pred = (weights[0] * y_pred_rf + weights[1] * y_pred_xgb + weights[2] * y_pred_logistic)
ensemble_pred_class = (ensemble_pred >= 0.5).astype(int)

# Evaluate the models
print("Random Forest Classification Report:\n", classification_report(y_test, (y_pred_rf >= 0.5).astype(int)))
print("XGBoost Classification Report:\n", classification_report(y_test, (y_pred_xgb >= 0.5).astype(int)))
print("Logistic Regression Classification Report:\n", classification_report(y_test, (y_pred_logistic >= 0.5).astype(int)))
print("Hybrid Model Classification Report:\n", classification_report(y_test, ensemble_pred_class))

# Calculate and print the accuracy scores
accuracy_rf = accuracy_score(y_test, (y_pred_rf >= 0.5).astype(int))
accuracy_xgb = accuracy_score(y_test, (y_pred_xgb >= 0.5).astype(int))
accuracy_logistic = accuracy_score(y_test, (y_pred_logistic >= 0.5).astype(int))
accuracy_ensemble = accuracy_score(y_test, ensemble_pred_class)

print(f"Random Forest Accuracy: {accuracy_rf:.2f}")
print(f"XGBoost Accuracy: {accuracy_xgb:.2f}")
print(f"Logistic Regression Accuracy: {accuracy_logistic:.2f}")
print(f"Hybrid Model Accuracy: {accuracy_ensemble:.2f}")

# Plot confusion matrix for hybrid model
conf_matrix = confusion_matrix(y_test, ensemble_pred_class)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.title('Confusion Matrix - Hybrid Model')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

# Plot feature importance from Random Forest
feature_importance_rf = pd.DataFrame({
    'Feature': X.columns,
    'Importance': random_forest_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_rf, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np

# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/post natal data.csv")  # Update the path as needed

# Drop unnecessary columns
data_cleaned = data.drop(columns=['Timestamp'])  # Drop Timestamp as it's not useful for prediction

# Handle missing values intelligently
# Fill numeric columns with the median and categorical columns with the mode
for column in data_cleaned.columns:
    if data_cleaned[column].dtype == 'object':
        data_cleaned[column] = data_cleaned[column].fillna(data_cleaned[column].mode()[0])
    else:
        data_cleaned[column] = data_cleaned[column].fillna(data_cleaned[column].median())

# Encode categorical data into numerical values
encoder = LabelEncoder()
for column in data_cleaned.columns:
    if data_cleaned[column].dtype == 'object':
        data_cleaned[column] = encoder.fit_transform(data_cleaned[column])

# Define features (X) and target (y)
X = data_cleaned.drop(columns=['Feeling sad or Tearful'])  # Replace target column as needed
y = data_cleaned['Feeling sad or Tearful']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=42)

# Initialize the models with low learning rates for high accuracy
random_forest_model = RandomForestClassifier(random_state=42, n_estimators=200, max_depth=10)
xgb_model = XGBClassifier(random_state=42, learning_rate=0.0001, n_estimators=200, eval_metric='logloss')
logistic_model = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')

# Train the models
random_forest_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)
logistic_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = random_forest_model.predict_proba(X_test)[:, 1]
y_pred_xgb = xgb_model.predict_proba(X_test)[:, 1]
y_pred_logistic = logistic_model.predict_proba(X_test)[:, 1]

# Hybrid model: Weighted average ensemble
weights = [0.4, 0.4, 0.2]  # Assign weights to each model
ensemble_pred = (weights[0] * y_pred_rf + weights[1] * y_pred_xgb + weights[2] * y_pred_logistic)
ensemble_pred_class = (ensemble_pred >= 0.5).astype(int)

# Evaluate the models
print("Random Forest Classification Report:\n", classification_report(y_test, (y_pred_rf >= 0.5).astype(int)))
print("XGBoost Classification Report:\n", classification_report(y_test, (y_pred_xgb >= 0.5).astype(int)))
print("Logistic Regression Classification Report:\n", classification_report(y_test, (y_pred_logistic >= 0.5).astype(int)))
print("Hybrid Model Classification Report:\n", classification_report(y_test, ensemble_pred_class))

# Calculate and print the accuracy scores
accuracy_rf = accuracy_score(y_test, (y_pred_rf >= 0.5).astype(int))
accuracy_xgb = accuracy_score(y_test, (y_pred_xgb >= 0.5).astype(int))
accuracy_logistic = accuracy_score(y_test, (y_pred_logistic >= 0.5).astype(int))
accuracy_ensemble = accuracy_score(y_test, ensemble_pred_class)

print(f"Random Forest Accuracy: {accuracy_rf:.2f}")
print(f"XGBoost Accuracy: {accuracy_xgb:.2f}")
print(f"Logistic Regression Accuracy: {accuracy_logistic:.2f}")
print(f"Hybrid Model Accuracy: {accuracy_ensemble:.2f}")

# Plot confusion matrix for hybrid model
conf_matrix = confusion_matrix(y_test, ensemble_pred_class)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.title('Confusion Matrix - Hybrid Model')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

# Plot feature importance from Random Forest
feature_importance_rf = pd.DataFrame({
    'Feature': X.columns,
    'Importance': random_forest_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_rf, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()